"""
Base class for LLM-powered agents.

All agents inherit from this and implement:
- _fetch_data(): What data to retrieve
- _build_prompt(): How to format prompt for LLM

DOMAIN AGNOSTIC: Agents can analyze ANY domain:
- Labor, Economy, Energy, Tourism, Health, Education, Trade, Technology, etc.
"""

import json
import logging
from abc import ABC, abstractmethod
from typing import AsyncIterator, Dict, Optional, List, Any
from datetime import datetime, timezone

from qnwis.agents.base import DataClient, AgentReport, Insight, evidence_from
from qnwis.llm.client import LLMClient
from qnwis.llm.parser import LLMResponseParser, AgentFinding
from qnwis.llm.exceptions import LLMError, LLMParseError
from qnwis.agents.data_mastery import get_agent_data_prompt, AGENT_DATA_MASTERY_PROMPT

logger = logging.getLogger(__name__)


# Zero Fabrication Citation Requirement
ZERO_FABRICATION_CITATION_RULES = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸš¨ ZERO FABRICATION POLICY - CRITICAL ðŸš¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You have been provided a NUMBERED LIST of verified facts (e.g., [FACT 1], [FACT 2]).
These are the ONLY numbers you are allowed to cite.

RULE 1: ONLY cite numbers from the [FACT N] list provided above.

RULE 2: Citation format:
  âœ… "ICT employment is [FACT 3: 2.1% from LMIS]"
  âœ… "GDP grew [FACT 7: 4.2% from World Bank]"

RULE 3: If a metric is NOT in the fact list:
  âœ… "NOT IN DATA - cannot provide ICT national participation rate"
  âœ… Provide QUALITATIVE analysis instead of fabricating numbers

RULE 4: FORBIDDEN - These will be DETECTED and REJECTED:
  âŒ "Per MoL LMIS 2023, nationals represent 0.8% of ICT" (NUMBER NOT IN LIST)
  âŒ "ICT employment grew 11.8% annually" (FABRICATED STATISTIC)
  âŒ "According to the data, productivity is 15% higher" (NO [FACT N] CITATION)

RULE 5: When you don't have data, say so:
  âœ… "Without specific ICT employment breakdowns in the data, I cannot quantify..."
  âœ… "The available data does not include national participation rates by sector..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â›” ANY NUMBER WITHOUT [FACT N] CITATION = FABRICATION = REJECTION â›”
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


# Decision-Focused Analysis Prompt - Prevents methodology tangents
DECISION_FOCUS_PROMPT = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CRITICAL: DECISION-FOCUSED ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You are advising a minister who needs to make a decision. Every response must:

1. DIRECTLY ADDRESS THE QUESTION: Your analysis must explicitly connect to the 
   original question. If asked "which path offers highest probability of success",
   your response must compare probabilities for different paths.

2. AVOID METHODOLOGY TANGENTS: Do NOT spend more than 2 sentences on:
   - Data classification debates (e.g., "whether data is sufficiently granular")
   - Economic theory explanations (e.g., "Leontief inverse coefficients")
   - Statistical methodology (e.g., "Input-Output table construction")
   
   If methodology is relevant, state your conclusion and move on:
   âŒ BAD: "The PSA ICT Satellite Account aggregates AI under 'Digital Transformation' 
           which makes isolation of AI-specific returns problematic because..."
   âœ… GOOD: "PSA data shows ICT grew 12% (though AI-specific data is limited). 
            This suggests..."

3. QUANTIFY WHEN POSSIBLE: Instead of "Technology offers better diversification", say:
   "Technology investment historically yields 2.3x higher productivity spillovers 
    than tourism (per World Bank estimates)."

4. PROGRESS THE DEBATE: Each turn must add NEW information or reach a NEW conclusion.
   Do not repeat points already made. If you agree, say "I agree with [Agent] on 
   [point], and I'll add [new insight]."

5. ANSWER THE ACTUAL QUESTION:
   - If asked to choose between options â†’ recommend one with probability/confidence
   - If asked for risks â†’ list specific risks with likelihood and impact
   - If asked for a strategy â†’ provide actionable steps with timeline
   - If asked an open question â†’ structure analysis around key dimensions

Remember: The minister doesn't care about methodology. They care about:
What should I do? Why? What are the risks? How confident are you?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


# Anti-Contradiction Prompt - Prevents data inconsistency spirals
ANTI_CONTRADICTION_PROMPT = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CRITICAL: DATA CONSISTENCY RULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

If you cite a statistic, you MUST use it consistently throughout the debate.

BEFORE citing any number:
1. Check: Have I or another agent already cited a different figure for this metric?
2. If YES: Either use the previously cited figure, OR explicitly acknowledge:
   "Earlier we cited X%, but updated data shows Y%. Using Y% going forward."

EXAMPLES:

âŒ WRONG - Contradicting without acknowledgment:
   Turn 23: "ICT employment grew 11.8% annually"
   Turn 40: "ICT employment grew only 0.8% per year"

âœ… RIGHT - Consistent usage:
   Turn 23: "ICT employment grew 11.8% annually (Per MoL LMIS 2023)"
   Turn 40: "Given the 11.8% growth rate cited earlier, we can project..."

âœ… RIGHT - Explicit correction:
   Turn 23: "ICT employment grew 11.8% annually"
   Turn 40: "While earlier data showed 11.8%, recent MoL 2024 data indicates 
            only 0.8%. Using the updated 0.8% figure..."

RULE: Maximum ONE turn to resolve any data discrepancy.
State which figure you trust, explain why briefly, then MOVE ON.

The minister doesn't care which growth rate is exactly right.
They care: Should we invest in AI or Tourism?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


class LLMAgent(ABC):
    """
    Base class for LLM-powered agents.
    
    Provides streaming execution with:
    - Data fetching from deterministic layer
    - LLM reasoning with structured output
    - Number validation against source data
    - Progress updates via streaming
    """
    
    def __init__(
        self,
        client: DataClient,
        llm: LLMClient
    ):
        """
        Initialize LLM agent.
        
        Args:
            client: Data client for deterministic queries
            llm: LLM client for generation
        """
        self.client = client
        self.llm = llm
        self.parser = LLMResponseParser()
        self.agent_name = self.__class__.__name__.replace("Agent", "")
    
    async def run_stream(
        self,
        question: str,
        context: Optional[Dict] = None,
        debate_context: str = ""
    ) -> AsyncIterator[dict]:
        """Run agent with streaming output and stage-level error events."""
        context = context or {}
        # Preserve existing context but ensure debate context is available downstream
        context.setdefault("debate_context", debate_context)
        start_time = datetime.now(timezone.utc)

        try:
            yield {"type": "stage", "stage": "data_fetch", "message": f"{self.agent_name} fetching data"}
            try:
                data = await self._fetch_data(question, context)
            except Exception as exc:
                logger.error("%s data fetch failed: %s", self.agent_name, exc, exc_info=True)
                yield {"type": "error", "content": f"Data fetch failed: {exc}"}
                return

            if not data:
                yield {"type": "warning", "content": f"?? {self.agent_name} found no relevant data"}
                empty_report = AgentReport(
                    agent=self.agent_name,
                    findings=[],
                    narrative="No relevant data found for this query."
                )
                latency_ms = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
                yield {"type": "complete", "report": empty_report, "latency_ms": latency_ms}
                return

            yield {"type": "stage", "stage": "prompt_build", "message": f"{self.agent_name} building prompt"}
            try:
                system_prompt, user_prompt = self._build_prompt(question, data, context)
            except Exception as exc:
                logger.error("%s prompt build failed: %s", self.agent_name, exc, exc_info=True)
                yield {"type": "error", "content": f"Prompt build failed: {exc}"}
                return

            yield {"type": "stage", "stage": "llm_call", "message": f"{self.agent_name} calling LLM"}
            response_text = ""
            token_count = 0
            try:
                async for token in self.llm.generate_stream(
                    prompt=user_prompt,
                    system=system_prompt,
                    temperature=0.3,
                    max_tokens=4096,  # Increased from 2000 to allow full analysis
                ):
                    response_text += token
                    token_count += 1
                    yield {"type": "token", "content": token}
            except LLMError as exc:
                logger.error("%s LLM call failed: %s", self.agent_name, exc, exc_info=True)
                yield {"type": "error", "content": f"LLM call failed: {exc}"}
                return

            logger.info("%s generated %d tokens in %.1fs", self.agent_name, token_count, (datetime.now(timezone.utc) - start_time).total_seconds())

            yield {"type": "stage", "stage": "parse", "message": f"{self.agent_name} parsing results"}
            try:
                finding = self.parser.parse_agent_response(response_text)
            except (LLMParseError, ValueError) as exc:
                logger.warning("%s JSON parse failed, falling back to raw text: %s", self.agent_name, exc)
                # Graceful degradation: Construct a finding from raw text
                # We assume the raw text contains the analysis in markdown
                finding = AgentFinding(
                     title=f"{self.agent_name} Analysis (Fallback)",
                     summary="Analysis provided below (JSON parse failed)",
                     metrics={},
                     analysis=response_text, # Raw text as narrative
                     recommendations=[],
                     confidence=0.5, # Low confidence due to parse failure
                     citations=[],
                     data_quality_notes=f"JSON parse failed - showing raw output. Error: {exc}"
                )
                yield {"type": "warning", "content": f"JSON parse failed - showing raw output"}

            allowed_numbers = self.parser.extract_numbers_from_query_results(data)
            is_valid, violations = self.parser.validate_numbers(finding, allowed_numbers, tolerance=0.02)

            if not is_valid:
                logger.debug("%s number validation: %d metric(s) inferred", self.agent_name, len(violations))

            insight = Insight(
                title=finding.title,
                summary=finding.summary,
                metrics=finding.metrics or {},
                evidence=[evidence_from(qr) for qr in data.values()],
                warnings=violations if not is_valid else [],
            )

            report = AgentReport(
                agent=self.agent_name,
                findings=[insight],
                narrative=finding.analysis,
            )

            latency_ms = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            logger.info("%s completed in %.0fms (confidence: %.2f)", self.agent_name, latency_ms, finding.confidence)

            yield {"type": "complete", "report": report, "latency_ms": latency_ms}

        except Exception as exc:
            logger.error("%s unexpected error: %s", self.agent_name, exc, exc_info=True)
            yield {"type": "error", "content": f"{self.agent_name} error: {exc}"}

    async def run(
        self,
        question: str,
        context: Optional[Dict] = None,
        debate_context: str = ""
    ) -> AgentReport:
        """Run agent without streaming while surfacing detailed error context."""
        context = context or {}
        logger.info("%s starting run()", self.agent_name)

        report: Optional[AgentReport] = None
        error_messages: list[str] = []

        try:
            async for event in self.run_stream(question, context, debate_context):
                event_type = event.get("type")
                if event_type == "complete":
                    report = event["report"]
                    break
                if event_type == "error":
                    message = event.get("content", "Unknown error")
                    error_messages.append(str(message))
                    logger.error("%s error event: %s", self.agent_name, message)
        except Exception as exc:
            logger.error("%s run_stream exception: %s", self.agent_name, exc, exc_info=True)
            raise RuntimeError(f"{self.agent_name} failed: {exc}") from exc

        if report is None:
            details = {
                "question_preview": (question or "")[:120],
                "context_keys": sorted(context.keys()),
                "errors": error_messages,
            }
            raise RuntimeError(
                f"{self.agent_name} failed to produce report. Details: {json.dumps(details, ensure_ascii=False)}"
            )

        logger.info("%s completed run()", self.agent_name)
        return report
    
    @abstractmethod
    async def _fetch_data(
        self,
        question: str,
        context: Dict
    ) -> Dict:
        """
        Fetch data needed for analysis.
        
        Must return dictionary of {key: QueryResult} from deterministic layer.
        
        Args:
            question: User's question
            context: Additional context
            
        Returns:
            Dictionary of QueryResults
        """
        pass
    
    @abstractmethod
    def _build_prompt(
        self,
        question: str,
        data: Dict,
        context: Dict
    ) -> tuple[str, str]:
        """
        Build agent-specific prompt for LLM.
        
        Args:
            question: User's question
            data: Dictionary of QueryResults
            context: Additional context
            
        Returns:
            (system_prompt, user_prompt) tuple
        """
        pass

    # --- Legendary Debate Conversation Methods ---

    async def present_case(self, topic: str, context: list, original_question: str = None) -> str:
        """Present opening statement on a topic with full data mastery."""
        
        # Get agent-specific data mastery knowledge
        data_mastery = get_agent_data_prompt(self.agent_name)
        
        # Use original_question if provided, else extract from topic
        question_display = original_question or topic[:500]
        
        # CONTENT FILTER SAFE: Uses soft language
        prompt = f"""THE MINISTER'S QUESTION:
"{question_display}"

{topic}

Based on your expertise as {self.agent_name}, analyze this query and present your position.

{DECISION_FOCUS_PROMPT}

{ANTI_CONTRADICTION_PROMPT}

Guidelines for your analysis:
- DIRECTLY address the minister's question above
- Use evidence from the data sources listed below
- Be specific: name the exact source and indicator code
- Focus on your domain of expertise
- Keep response concise (2-3 paragraphs)
- Cite facts as: "[Per SOURCE: value]"
- Take a CLEAR POSITION - do not hedge

{data_mastery}

Your expert analysis (addressing the minister's question directly):"""
        
        return await self.llm.generate(
            prompt=prompt,
            system=f"You are {self.agent_name}, providing decision-focused expert analysis. The minister needs a recommendation, not methodology discussion.",
            temperature=0.3,
            max_tokens=1500  # Increased from 500 to prevent truncation
        )

    async def challenge_position(
        self, 
        opponent_name: str,
        opponent_claim: str,
        conversation_history: list,
        turn_number: int = 0,
        total_turns: int = 10,
        phase: str = "debate",
        original_question: str = None
    ) -> str:
        """Challenge another agent's position with data-backed evidence and structured debate rules."""
        history_text = self._format_history(conversation_history[-5:])
        
        # Extract original query AND facts context from conversation history
        original_query = "Policy analysis"
        extracted_facts_context = ""
        
        if conversation_history and len(conversation_history) > 0:
            # Check first turn for injected context (from Moderator)
            first_turn = conversation_history[0]
            if isinstance(first_turn, dict):
                msg = first_turn.get('message', '')
                agent = first_turn.get('agent', '')
                
                # If first turn is the injected context from Moderator
                if agent == 'Moderator' and 'THE QUESTION BEING ANALYZED' in msg:
                    # Extract query
                    if 'THE QUESTION BEING ANALYZED' in msg:
                        parts = msg.split('THE QUESTION BEING ANALYZED')
                        if len(parts) > 1:
                            query_section = parts[1].split('=')[0].strip()
                            if query_section:
                                original_query = query_section[:500]
                    
                    # Extract facts section
                    if 'VERIFIED FACTS' in msg:
                        facts_start = msg.find('VERIFIED FACTS')
                        facts_end = msg.find('CITATION REQUIREMENT')
                        if facts_start > 0 and facts_end > facts_start:
                            extracted_facts_context = msg[facts_start:facts_end]
                
                # Fallback: look for QUERY BEING ANALYZED in any message
                elif 'QUERY BEING ANALYZED:' in msg:
                    lines = msg.split('\n')
                    for i, line in enumerate(lines):
                        if 'QUERY BEING ANALYZED:' in line and i + 1 < len(lines):
                            original_query = lines[i + 1].strip()
                            break
        
        # Get data mastery for evidence-based challenges
        data_mastery = get_agent_data_prompt(self.agent_name)
        
        # Build prompt with extracted facts if available
        # CONTENT FILTER SAFE: Uses soft language
        facts_section = ""
        if extracted_facts_context:
            facts_section = f"""
Available extracted facts for reference:
{extracted_facts_context}
"""
        
        # Use passed original_question if available, else use extracted
        question_to_use = original_question if original_question else original_query
        
        prompt = f"""You are {self.agent_name}.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
THE MINISTER'S QUESTION (STAY FOCUSED ON THIS):
"{question_to_use}"
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{DECISION_FOCUS_PROMPT}

{ANTI_CONTRADICTION_PROMPT}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEBATE CONTEXT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- Turn: {turn_number} of {total_turns}
- Phase: {phase}
- Your role: Challenge {opponent_name}'s position

DEBATE RULES (FOLLOW STRICTLY):
1. You MUST engage with SPECIFIC points from {opponent_name}'s statement
2. If you agree on some points, say so - then add NEW evidence
3. If you disagree, cite SPECIFIC data that contradicts their claim
4. Do NOT repeat points already made in the conversation
5. Build toward synthesis - identify what would resolve the disagreement
6. ALWAYS connect your challenge back to the minister's question
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{facts_section}
{opponent_name} stated: "{opponent_claim[:400]}..."

Recent conversation:
{history_text}

Available data sources:
{data_mastery[:1500]}

Your challenge must:
1. Quote the SPECIFIC claim you're challenging
2. Present counter-evidence with citations [Per extraction: 'value' from source]
3. Explain WHY your evidence contradicts their position
4. Connect your challenge to the minister's decision

Your challenge (focused on the minister's question):"""
        
        return await self.llm.generate(
            prompt=prompt,
            system=f"You are {self.agent_name}, engaging in decision-focused debate. Every challenge must help answer the minister's question.",
            temperature=0.4,
            max_tokens=1500  # Increased from 500 to prevent truncation
        )

    async def respond_to_challenge(
        self,
        challenger_name: str, 
        challenge: str,
        conversation_history: list,
        turn_number: int = 0,
        total_turns: int = 10,
        phase: str = "debate",
        original_question: str = None
    ) -> str:
        """Respond to a challenge with data-backed evidence and structured debate rules."""
        history_text = self._format_history(conversation_history[-5:])
        
        # Extract original query AND facts context from conversation history
        original_query = "Policy analysis"
        extracted_facts_context = ""
        
        if conversation_history and len(conversation_history) > 0:
            first_turn = conversation_history[0]
            if isinstance(first_turn, dict):
                msg = first_turn.get('message', '')
                agent = first_turn.get('agent', '')
                
                if agent == 'Moderator' and 'THE QUESTION BEING ANALYZED' in msg:
                    # Extract query
                    parts = msg.split('THE QUESTION BEING ANALYZED')
                    if len(parts) > 1:
                        query_section = parts[1].split('=')[0].strip()
                        if query_section:
                            original_query = query_section[:500]
                    
                    # Extract facts section
                    if 'VERIFIED FACTS' in msg:
                        facts_start = msg.find('VERIFIED FACTS')
                        facts_end = msg.find('CITATION REQUIREMENT')
                        if facts_start > 0 and facts_end > facts_start:
                            extracted_facts_context = msg[facts_start:facts_end]
        
        # Get data mastery for evidence-based defense
        data_mastery = get_agent_data_prompt(self.agent_name)
        
        # Build prompt with extracted facts if available
        # CONTENT FILTER SAFE: Uses soft language
        facts_section = ""
        if extracted_facts_context:
            facts_section = f"""
Available extracted facts for reference:
{extracted_facts_context}
"""
        
        # Use passed original_question if available, else use extracted
        question_to_use = original_question if original_question else original_query
        
        prompt = f"""You are {self.agent_name}.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
THE MINISTER'S QUESTION (STAY FOCUSED ON THIS):
"{question_to_use}"
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{DECISION_FOCUS_PROMPT}

{ANTI_CONTRADICTION_PROMPT}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEBATE CONTEXT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- Turn: {turn_number} of {total_turns}
- Phase: {phase}
- Your role: Respond to {challenger_name}'s challenge

DEBATE RULES (FOLLOW STRICTLY):
1. You MUST address the SPECIFIC points {challenger_name} raised
2. Acknowledge valid criticisms honestly - "I concede that..."
3. Defend your position with NEW evidence not yet presented
4. Do NOT repeat your original argument - add to it or modify it
5. Build toward synthesis - where can you find common ground?
6. ALWAYS connect your response to the minister's question
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{facts_section}
{challenger_name} challenged you: "{challenge[:400]}"

Recent conversation:
{history_text}

Available data sources to support your position:
{data_mastery[:1500]}

Your response must:
1. Quote the SPECIFIC criticism you're addressing
2. Either CONCEDE (if valid) or DEFEND with new evidence
3. Cite all data: [Per extraction: 'value' from source]
4. Connect your defense to the minister's decision

Structure your response:
- "On [specific point], I [concede/maintain]..."
- "The evidence shows [Per extraction: data]..."
- "For the minister's decision, this means..."

Your response (focused on the minister's question):"""
        
        return await self.llm.generate(
            prompt=prompt,
            system=f"You are {self.agent_name}, engaging in decision-focused debate. Every response must help answer the minister's question.",
            temperature=0.3,
            max_tokens=1500  # Increased from 500 to prevent truncation
        )

    async def contribute_to_discussion(
        self,
        conversation_history: list,
        original_question: str = None
    ) -> str:
        """Contribute to ongoing discussion with new data perspectives."""
        history_text = self._format_history(conversation_history[-8:])
        
        # Get data mastery for evidence-based contributions
        data_mastery = get_agent_data_prompt(self.agent_name)
        
        # Extract original query from history if not provided
        extracted_query = "Policy analysis"
        if original_question:
            extracted_query = original_question
        elif conversation_history and len(conversation_history) > 0:
            first_turn = conversation_history[0]
            if isinstance(first_turn, dict):
                msg = first_turn.get('message', '')
                if 'THE QUESTION BEING ANALYZED' in msg or 'TOPIC LOCK' in msg:
                    # Try to extract the question
                    lines = msg.split('\n')
                    for i, line in enumerate(lines):
                        if 'TOPIC LOCK' in line or 'QUESTION' in line.upper():
                            # Next few lines likely contain the question
                            for j in range(i+1, min(i+5, len(lines))):
                                if len(lines[j].strip()) > 50:
                                    extracted_query = lines[j].strip()[:500]
                                    break
                            break
        
        prompt = f"""You are {self.agent_name}.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
THE MINISTER'S QUESTION (STAY FOCUSED ON THIS):
"{extracted_query}"
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{DECISION_FOCUS_PROMPT}

{ANTI_CONTRADICTION_PROMPT}

Ongoing debate:
{history_text}

YOU HAVE ACCESS TO THESE DATA SOURCES - BRING NEW DATA TO THE DISCUSSION:
{data_mastery[:2000]}

Contribute your perspective by:
- Offering insights that DIRECTLY help answer the minister's question
- Presenting ADDITIONAL EVIDENCE from sources NOT YET DISCUSSED
- Proposing synthesis backed by DATA
- Making a CLEAR RECOMMENDATION with confidence level

DO NOT discuss methodology unless directly relevant to the decision.

Your contribution (WITH DATA CITATIONS, focused on the minister's decision):"""
        
        return await self.llm.generate(
            prompt=prompt,
            system=f"You are {self.agent_name}, contributing decision-focused expertise. Every contribution must help the minister decide.",
            temperature=0.4,
            max_tokens=1200  # Increased to allow complete responses
        )

    async def analyze_edge_case(
        self, 
        scenario: Dict,
        conversation_history: List[Dict]
    ) -> str:
        """Analyze edge case scenario from domain perspective."""
        
        prompt = f"""You are {self.agent_name}.

Edge Case Scenario:
- Description: {scenario.get('description', 'Unknown')}
- Severity: {scenario.get('severity', 'Unknown')}
- Probability: {scenario.get('probability_pct', 'Unknown')}%

Based on your domain expertise and the debate so far, analyze:
1. How would this scenario impact your domain?
2. Would your recommendations still hold?
3. What contingency measures are needed?
4. What early warning indicators should we monitor?

Be specific with numbers and timelines."""
        
        return await self.llm.generate(prompt=prompt, temperature=0.4, max_tokens=1500)

    async def identify_catastrophic_risks(
        self,
        conversation_history: List[Dict],
        mode: str = "pessimistic",
        query_context: str = ""
    ) -> str:
        """
        Identify risks specific to the decision being debated.
        
        Note: Prompts use soft language to avoid Azure content filter false positives.
        Avoids: "CRITICAL INSTRUCTION", "DO NOT", "Focus ONLY", uppercase blocks.
        """
        
        debate_summary = self._summarize_recent_debate(conversation_history)
        
        # Extract original query from conversation history
        original_query = ""
        if conversation_history and len(conversation_history) > 0:
            first_turn = conversation_history[0]
            if isinstance(first_turn, dict):
                msg = first_turn.get('message', '')
                if 'THE QUESTION BEING ANALYZED' in msg:
                    parts = msg.split('THE QUESTION BEING ANALYZED')
                    if len(parts) > 1:
                        original_query = parts[1].split('=')[0].strip()[:400]
        
        # Use query_context if provided, otherwise use extracted query
        context_to_use = query_context if query_context else original_query
        
        # CONTENT FILTER SAFE: Uses soft, conversational language
        # Avoids uppercase commands, "DO NOT", "CRITICAL", etc.
        prompt = f"""You are {self.agent_name}, a policy analyst providing strategic risk assessment.

The decision being analyzed:
{context_to_use}

Previous discussion summary:
{debate_summary}

Please analyze the specific risks related to the decision options above.
Keep your analysis focused on the options presented rather than general global trends.

For each option in this decision, please assess:
1. The primary risk if this option is chosen
2. Key assumptions that might prove incorrect
3. Competitive challenges that could affect success
4. A realistic downside scenario

Please provide constructive analysis based on the available data.
Keep your response practical and focused on this specific decision."""
        
        return await self.llm.generate(prompt=prompt, temperature=0.4, max_tokens=600)

    async def assess_risk_likelihood(
        self,
        risk_description: str,
        conversation_history: List[Dict]
    ) -> str:
        """Assess likelihood and impact of identified risk."""
        
        # Sanitize risk description to prevent content filter triggers
        sanitized_risk = self._sanitize_for_azure(risk_description[:500])
        
        # CONTENT FILTER SAFE: Uses soft language
        prompt = f"""You are {self.agent_name}, providing strategic assessment.

Another analyst identified this potential concern:
{sanitized_risk}

Please assess from your domain perspective:
1. Likelihood - estimated probability in next 2 years (as percentage)
2. Impact severity if it occurs (scale of 1-10)
3. Early warning indicators to monitor
4. Potential mitigation strategies
5. Your confidence level in this assessment

Please provide an objective and balanced assessment."""
        
        return await self.llm.generate(prompt=prompt, temperature=0.3)

    async def state_final_position(
        self,
        debate_history: List[Dict],
        confidence_level: bool = True
    ) -> str:
        """State final position after debate."""
        history_text = self._format_history(debate_history[-10:])
        
        prompt = f"""You are {self.agent_name}.

The debate is concluding.
Recent conversation:
{history_text}

State your final position:
1. Your core recommendation
2. Key caveats or risks
3. Your final confidence level (0-100%)
4. What would change your mind?

Be decisive."""
        
        return await self.llm.generate(prompt=prompt, temperature=0.3)

    def _format_history(self, history: list) -> str:
        """Format conversation history for prompts."""
        lines = []
        for turn in history:
            agent = turn.get("agent", "Unknown")
            message = turn.get("message", "")
            # Sanitize message to avoid content filter triggers
            message = self._sanitize_for_azure(message[:200])
            lines.append(f"{agent}: {message}...")
        return "\n".join(lines)
    
    def _sanitize_for_azure(self, text: str) -> str:
        """
        Sanitize text to prevent Azure content filter false positives.
        
        Root cause: Azure's jailbreak detection triggers on patterns that
        look like instruction injection or adversarial prompts, but are
        actually legitimate debate/analysis content.
        """
        if not text:
            return text
        
        # Patterns that trigger Azure's jailbreak detection in policy analysis
        replacements = {
            # Role-playing (looks like prompt injection)
            "act as": "analyze as",
            "pretend to be": "consider from perspective of",
            "play the role": "provide analysis as",
            "devil's advocate": "alternative perspective",
            "Devil's Advocate": "Alternative Perspective",
            # Adversarial framing
            "attack the": "examine the",
            "attack this": "examine this",
            "attack argument": "examine argument",
            "exploit": "leverage",
            "manipulate": "influence",
            # Command-like patterns
            "ignore previous": "also consider",
            "ignore all": "additionally",
            "disregard": "also consider",
            "override": "supplement",
            "bypass": "work around",
            # Risk language that may trigger
            "catastrophic": "significant",
            "nightmare": "challenging",
            "worst-case": "downside",
            "destroy": "significantly impact",
            # Policy-specific triggers
            "undermine": "challenge",
            "subvert": "question",
            "circumvent": "address",
        }
        
        result = text
        for old, new in replacements.items():
            result = result.replace(old, new)
            # Also handle case variations
            result = result.replace(old.lower(), new.lower())
            result = result.replace(old.upper(), new.upper())
        
        return result

    def _should_contribute(self, conversation_history: list) -> bool:
        """Decide if this agent should contribute to discussion."""
        # Contribute if: relevant keywords in recent conversation
        # For MVP, use simple keyword matching
        history_text = self._format_history(conversation_history[-3:])
        keywords = self._get_agent_keywords()
        
        return any(keyword.lower() in history_text.lower() for keyword in keywords)

    def _get_agent_keywords(self) -> list:
        """Get keywords relevant to this agent."""
        # Override in subclasses for better relevance detection
        return [self.agent_name.lower()]

    def _summarize_recent_debate(self, history: list) -> str:
        """Summarize recent debate turns."""
        if not history:
            return "No prior debate."
        return self._format_history(history[-10:])
